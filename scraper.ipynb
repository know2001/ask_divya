{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/know2001/ask_divya/blob/dani-in_progress/dani_scraper_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FhS9CjkI6H-"
      },
      "source": [
        "# BeautifulSoup4 Scraper Notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dblEmx_aNp2F"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import urllib.parse\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "932ddE1BI3f8"
      },
      "source": [
        "## Get Soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xhzHY-shOdO9"
      },
      "outputs": [],
      "source": [
        "def get_soup(url):\n",
        "    page = requests.get(url) # gets status code of a web page\n",
        "    soup = BeautifulSoup(page.text, 'html.parser') # Parsed HTML code\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChVyO9bU2cKB"
      },
      "source": [
        "## Get Links (Simple Spider)\n",
        "BS4 has two search methods, `find()` and `find_all()`. The first will give you the first element that meets the search condition, the later will give you a list of all the findings. In HTML the a tag defines a hyperlink, in this case we want to fish all the urls that have the base url in common, to get all the documentation about immigration.\n",
        "\n",
        "\n",
        "The url is stored by the href attribute. It is worth noting that in HTML you use HTML's `<base>` tag to specify the base url for all elements that use the `href` attribute. Now, any tag with an `href` or `src` attribute that is empty, it will automatically go to the url you specified in the base tag by default. We are also going to parse the urls extracted from all the hyperlinks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOTQ6dsv9scd"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> url ='https://cat.example/list;meow?breed=siberian#pawsize'\n",
        ">>> urllib.parse.urlparse(url)\n",
        "ParseResult(scheme='https', netloc='cat.example', path='/list', params='meow', query='breed=siberian', fragment='pawsize')\n",
        "```\n",
        "```\n",
        ">>> url ='https://cat.example/list;meow?breed=siberian#pawsize'\n",
        ">>> parsed_url = urllib.parse.urlparse(url)\n",
        ">>> parsed_url.fragment\n",
        "pawsize\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj4ke7yH_mEJ"
      },
      "source": [
        "One the url parser joins the base url only if the relative url is missing one in the scheme. There are some urls that are already absolute and do have a base url. Usually the href URLs are relative. In that case `urllib.parser.urljoin()` will not join a new base_url. Check the next two examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BruTWLceo1Vx"
      },
      "source": [
        "When they both have a base url, and it differs, url2 keeps its base url:\n",
        "```\n",
        "urllib.parse.urljoin('http://BASE_URL1/%7Eguido/Python.html', 'http://BASE_URL2/FAQ.html')\n",
        ">>> http://BASE_URL2/FAQ.html\n",
        "```\n",
        "When the second url is relative, it acquires the base url from url1\n",
        "```\n",
        "urllib.parse.urljoin('http://BASE_URL1/%7Eguido/Python.html', 'FAQ.html')\n",
        ">>> http://BASE_URL1/%7Eguido/FAQ.html\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wZfh7nr6OgZV"
      },
      "outputs": [],
      "source": [
        "def get_links(soup, base_url):\n",
        "    links = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        url = link[\"href\"] # get url from href attribute\n",
        "        # Resolve relative links\n",
        "        url = urllib.parse.urljoin(base_url, url) #joins relative link to base_url\n",
        "        # Avoid repeating links just because they have a fragment\n",
        "        fragment = urllib.parse.urlparse(url).fragment\n",
        "        url = url.replace(('#'+fragment),'')\n",
        "        if url.startswith(base_url) and url not in links:\n",
        "            links.append(url)\n",
        "    return links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwPIdFXyhIpA"
      },
      "source": [
        "## Get Page Title\n",
        "It could be useful to get the title for the contents we are going to collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IanxYCoOhko6"
      },
      "outputs": [],
      "source": [
        "def get_title(soup):\n",
        "    title = soup.find('h1').text.strip()\n",
        "    return title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRsxcS6BJ7Xt"
      },
      "source": [
        "## Get Text\n",
        "what text?\n",
        "- The Main Header of the page h1\n",
        "- Text inside the Main element extracting the text from:\n",
        "    - The subheaders: h2...h6\n",
        "    - The paragraphs: p\n",
        "    - The accordion__headers and paragraphs \n",
        "        ```\n",
        "        >>> soup = get_soup('https://www.uscis.gov/working-in-the-united-states/\n",
        "        >>> temporary-workers/e-1-treaty-traders')\n",
        "        >>> acc_headers = soup.find_all('div',class_='accordion__header cke-active')\n",
        "        >>> for i in acc_headers:\n",
        "        >>>    print(i['class'],i.get_text) # see how the class key has is a list of two values\n",
        "        ['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">Who May File for Change of Status to E-1 Classification</div>>\n",
        "        ['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">How to Obtain E-1 Classification if Outside the United States</div>>\n",
        "        ...\n",
        "        ```\n",
        "    - The ordered lists and unordered lists: ol and ul\n",
        "    - The tables. Extracting the text of each cell and prepending the header of each column to it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_svc1HZFJ7Cp"
      },
      "outputs": [],
      "source": [
        "def get_table_text(table):\n",
        "    text = ''\n",
        "    table_headers = [header.get_text() for header in table.find_all('th')]\n",
        "    table_rows = [row for row in table.find_all('tr')]\n",
        "\n",
        "    for row in table_rows:\n",
        "        row_cells = [cell.get_text().strip() for cell in row.find_all('td')]\n",
        "        row_text =''\n",
        "        for i, cell_text in enumerate(row_cells):\n",
        "            cell_text = ' '.join([table_headers[i], cell_text])\n",
        "            row_text =  ' '.join([row_text, cell_text]).replace('\\n', '')\n",
        "\n",
        "        text = '\\n'.join([text, row_text])\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_row(url, soup):\n",
        "    title = get_title(soup)\n",
        "    url = url\n",
        "    text = ''\n",
        "    main = soup.find('main')\n",
        "\n",
        "    for element in main.find_all():\n",
        "        if element.name in ['ul', 'p', 'ol', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "            text = '\\n'.join([text, element.get_text()])\n",
        "        elif element.has_attr('class'):\n",
        "            if len(element['class'])>1:\n",
        "                if element['class'][0]=='accordion__header':\n",
        "                    text = '\\n'.join([text, element.get_text()])\n",
        "        elif element.name == 'table':\n",
        "            table_text = get_table_text(element)\n",
        "            text = '\\n'.join([text, table_text])\n",
        "\n",
        "\n",
        "    row = [title, url, text]\n",
        "    return row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataFrame rows by Section\n",
        "This function stores the scraped text of each page distinguishing the sections within a page, each section will be stored in a different row. It does not differentiate by subsections, only sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_rows(url, soup):\n",
        "    title = get_title(soup)\n",
        "    rows = [] #for the dataframe\n",
        "    url = url\n",
        "    main = soup.find('main')\n",
        "    section_text =''\n",
        "    section_title = ''\n",
        "\n",
        "    for element in main.find_all():\n",
        "        if element.name == 'h2':\n",
        "            if section_title != '':\n",
        "                rows.append([url, title, section_title, section_text]) # the section is saved as a row for the df with all the scrapped text\n",
        "                section_text =''\n",
        "            section_title = element.get_text()\n",
        "        elif element.name in ['ul', 'p', 'ol']:\n",
        "            section_text = '\\n'.join([section_text, element.get_text()])\n",
        "        elif element.has_attr('class'):\n",
        "            if len(element['class'])>1:\n",
        "                if element['class'][0]=='accordion__header':\n",
        "                    section_text = '\\n'.join([section_text, element.get_text()])\n",
        "        elif element.name == 'table':\n",
        "            table_text = get_table_text(element)\n",
        "            section_text = '\\n'.join([section_text, table_text])\n",
        "    \n",
        "    # Append the only section or the last section\n",
        "    if section_title == '': \n",
        "        section_title = title\n",
        "        rows.append([url, title, section_title, section_text]) # no subsections\n",
        "    else: \n",
        "        rows.append([url, title, section_title, section_text]) # last section\n",
        "    return rows        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg55IclULH5_"
      },
      "source": [
        "## Scraper\n",
        "The scraper is going to visit all the relative urls and extract the useful contents from the paragraphs of each page. It will write the text on an output file, a csv.\n",
        "\n",
        "When you open a file you usually use with open(), this method will automatically close the file after you are done reading or writing. Open takes three attributes, the file name, the mode, and the encoding (automatic). You are usually reading or writing on a file, `r` will select reading mode, `w` will select writing mode. It is worth mentioning the modes:\n",
        "*   w+: Opens a file in read and write mode. It creates a new file if it does not exist, if it exists, it erases the contents of the file and the file pointer starts from the beginning.\n",
        "*   rw+: Opens a file in read and write mode. File pointer starts at the beginning of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scraper(base_url, output_file):\n",
        "    visited = set()\n",
        "    to_visit = [base_url]\n",
        "    data = []\n",
        "\n",
        "    i=0\n",
        "    while to_visit:\n",
        "        # get url from to_visit\n",
        "        url = to_visit.pop() # removes and returns last element of the list\n",
        "        # confirm it is not in visited if it is skip to next iteration using continue\n",
        "        if url in visited:\n",
        "            continue\n",
        "        i+=1\n",
        "        if i%5==0:\n",
        "            print(f'{i} pages scraped')\n",
        "        # add to visited\n",
        "        visited.add(url)\n",
        "        # get soup\n",
        "        soup = get_soup(url)\n",
        "        # get page title and text from soup and create a new row which has dict format\n",
        "        rows = get_rows(url, soup)\n",
        "        # write new row in the csv\n",
        "        data.extend(rows)\n",
        "        # get links from soup\n",
        "        links = get_links(soup, base_url)\n",
        "        # append links to to_visit list if they are not in the visited set\n",
        "        to_visit.extend(link for link in links if link not in visited)\n",
        "\n",
        "    print(f'{i} Pages scraped in total')\n",
        "    columns = ['url', 'title', 'section', 'text']\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pjDg-S6rFg0a"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://www.uscis.gov/working-in-the-united-states\"\n",
        "output_file = \"text.csv\"\n",
        "soup = get_soup(base_url)\n",
        "links = get_links(soup, base_url)\n",
        "df = scraper(base_url, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukkIrPOEmzhx"
      },
      "source": [
        "Check for duplicate data in the text column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeIvo596m2-U",
        "outputId": "9b4abcfa-1628-440d-b00a-389e206df5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 duplicates\n"
          ]
        }
      ],
      "source": [
        "print(f\"{sum(df.duplicated(subset='text'))} duplicates\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1I4sXO6mUAe"
      },
      "source": [
        "## Save CSV\n",
        "We also want to make sure we can save the DataFrame into a CSV, and then load it back without breaking the shape (row x column)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aNG-1waFM-Pk",
        "outputId": "f6f03616-f853-4e95-95ef-880f32a0d957"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>section</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>Working in the United States</td>\n",
              "      <td>Topics</td>\n",
              "      <td>\\nMany noncitizens want to come to the United ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>Petition Process Overview</td>\n",
              "      <td>Form I-129, Petition for Nonimmigrant Worker</td>\n",
              "      <td>\\nIf you would like to come to the United Stat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>Petition Process Overview</td>\n",
              "      <td>Form I-140, Immigrant Petition for Alien Workers</td>\n",
              "      <td>\\nBelow is the list of Form I-140, Immigrant P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>Petition Process Overview</td>\n",
              "      <td>Form I-360, Petition for Amerasian, Widow(er),...</td>\n",
              "      <td>\\nBelow is the list of Form I-360, Petition fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>Petition Process Overview</td>\n",
              "      <td>Form I-526, Immigrant Petition by Alien Investor</td>\n",
              "      <td>\\nBelow is the list of Form I-526, Immigrant P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>EB-5 What's New</td>\n",
              "      <td>EB-5 What's New</td>\n",
              "      <td>\\nThis page provides the latest information on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>EB-5 Regional Center Compliance Reviews</td>\n",
              "      <td>Compliance Review Team Tasks</td>\n",
              "      <td>\\nThis page in Simplified Chinese.  (PDF, 95.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>EB-5 Regional Center Compliance Reviews</td>\n",
              "      <td>Preparing for a Compliance Review</td>\n",
              "      <td>\\nBefore the site assessment, regional centers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>EB-5 Regional Center Compliance Reviews</td>\n",
              "      <td>After Completing the Review</td>\n",
              "      <td>\\nThe review team will document the results in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>https://www.uscis.gov/working-in-the-united-st...</td>\n",
              "      <td>EB-5 Regional Center Compliance Reviews</td>\n",
              "      <td>Participating in a Compliance Review</td>\n",
              "      <td>\\nPlease note regional centers must continue t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>199 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   url  \\\n",
              "0    https://www.uscis.gov/working-in-the-united-st...   \n",
              "1    https://www.uscis.gov/working-in-the-united-st...   \n",
              "2    https://www.uscis.gov/working-in-the-united-st...   \n",
              "3    https://www.uscis.gov/working-in-the-united-st...   \n",
              "4    https://www.uscis.gov/working-in-the-united-st...   \n",
              "..                                                 ...   \n",
              "194  https://www.uscis.gov/working-in-the-united-st...   \n",
              "195  https://www.uscis.gov/working-in-the-united-st...   \n",
              "196  https://www.uscis.gov/working-in-the-united-st...   \n",
              "197  https://www.uscis.gov/working-in-the-united-st...   \n",
              "198  https://www.uscis.gov/working-in-the-united-st...   \n",
              "\n",
              "                                       title  \\\n",
              "0               Working in the United States   \n",
              "1                  Petition Process Overview   \n",
              "2                  Petition Process Overview   \n",
              "3                  Petition Process Overview   \n",
              "4                  Petition Process Overview   \n",
              "..                                       ...   \n",
              "194                          EB-5 What's New   \n",
              "195  EB-5 Regional Center Compliance Reviews   \n",
              "196  EB-5 Regional Center Compliance Reviews   \n",
              "197  EB-5 Regional Center Compliance Reviews   \n",
              "198  EB-5 Regional Center Compliance Reviews   \n",
              "\n",
              "                                               section  \\\n",
              "0                                               Topics   \n",
              "1         Form I-129, Petition for Nonimmigrant Worker   \n",
              "2     Form I-140, Immigrant Petition for Alien Workers   \n",
              "3    Form I-360, Petition for Amerasian, Widow(er),...   \n",
              "4     Form I-526, Immigrant Petition by Alien Investor   \n",
              "..                                                 ...   \n",
              "194                                    EB-5 What's New   \n",
              "195                       Compliance Review Team Tasks   \n",
              "196                  Preparing for a Compliance Review   \n",
              "197                        After Completing the Review   \n",
              "198               Participating in a Compliance Review   \n",
              "\n",
              "                                                  text  \n",
              "0    \\nMany noncitizens want to come to the United ...  \n",
              "1    \\nIf you would like to come to the United Stat...  \n",
              "2    \\nBelow is the list of Form I-140, Immigrant P...  \n",
              "3    \\nBelow is the list of Form I-360, Petition fo...  \n",
              "4    \\nBelow is the list of Form I-526, Immigrant P...  \n",
              "..                                                 ...  \n",
              "194  \\nThis page provides the latest information on...  \n",
              "195  \\nThis page in Simplified Chinese.  (PDF, 95.3...  \n",
              "196  \\nBefore the site assessment, regional centers...  \n",
              "197  \\nThe review team will document the results in...  \n",
              "198  \\nPlease note regional centers must continue t...  \n",
              "\n",
              "[199 rows x 4 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.to_csv('scraped_sections.csv', index = False)\n",
        "df1 = pd.read_csv('scraped_sections.csv')\n",
        "df2 = pd.read_csv('scraped_data.csv')\n",
        "df1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNovoG9W1dtszh/YVLcF+9f",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
