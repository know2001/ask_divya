{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVDjI6rF+kFO08JPsPMoQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/know2001/ask_divya/blob/dani-in_progress/dani_scraper_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BeautifulSoup4 Scraper Notes"
      ],
      "metadata": {
        "id": "4FhS9CjkI6H-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZiDSAfMNTCk",
        "outputId": "0fd2c129-f911-4bd5-9c3e-7d99ee437e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import urllib.parse\n",
        "import pandas as pd\n",
        "import csv"
      ],
      "metadata": {
        "id": "dblEmx_aNp2F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Soup"
      ],
      "metadata": {
        "id": "932ddE1BI3f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soup(url):\n",
        "    page = requests.get(url) # gets status code of a web page\n",
        "    soup = BeautifulSoup(page.text, 'html.parser') # Parsed HTML code\n",
        "    return soup"
      ],
      "metadata": {
        "id": "xhzHY-shOdO9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Links (Simple Spider)\n",
        "BS4 has two search methods, `find()` and `find_all()`. The first will give you the first element that meets the search condition, the later will give you a list of all the findings. In HTML the a tag defines a hyperlink, in this case we want to fish all the urls that have the base url in common, to get all the documentation about immigration.\n",
        "\n",
        "\n",
        "The url is stored by the href attribute. It is worth noting that in HTML you use HTML's `<base>` tag to specify the base url for all elements that use the `href` attribute. Now, any tag with an `href` or `src` attribute that is empty, it will automatically go to the url you specified in the base tag by default. We are also going to parse the urls extracted from all the hyperlinks:"
      ],
      "metadata": {
        "id": "ChVyO9bU2cKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> url ='https://cat.example/list;meow?breed=siberian#pawsize'\n",
        ">>> urllib.parse.urlparse(url)\n",
        "ParseResult(scheme='https', netloc='cat.example', path='/list', params='meow', query='breed=siberian', fragment='pawsize')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iOTQ6dsv9scd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One the url parser joins the base url only if the relative url is missing one in the scheme. There are some urls that are already absolute and do have a base url. In that case `urllib.parser.urljoin()` will not join a new base_url. Check the next two examples:"
      ],
      "metadata": {
        "id": "wj4ke7yH_mEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.parse.urljoin('http://BASE_URL1/%7Eguido/Python.html', 'http://BASE_URL2/FAQ.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YefR8_SF-9L1",
        "outputId": "f9e9ccf6-fd0a-4067-d15f-89b026018e31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://BASE_URL2/FAQ.html'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.parse.urljoin('http://BASE_URL1/%7Eguido/Python.html', 'FAQ.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KBNrADAq-5QI",
        "outputId": "ca563d56-bff7-406f-a984-296cd25a88ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://BASE_URL1/%7Eguido/FAQ.html'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_links(soup, base_url):\n",
        "    links = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        url = link[\"href\"] # get url from href attribute\n",
        "        # Resolve relative links\n",
        "        url = urllib.parse.urljoin(base_url, url) #joins relative link to base_url\n",
        "        if url.startswith(base_url) and url not in links:\n",
        "            links.append(url)\n",
        "    return links"
      ],
      "metadata": {
        "id": "wZfh7nr6OgZV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.uscis.gov/working-in-the-united-states\"\n",
        "output_file = \"text.csv\"\n",
        "soup = get_soup(base_url)\n",
        "links = get_links(soup, base_url)"
      ],
      "metadata": {
        "id": "pjDg-S6rFg0a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Page Title\n",
        "It could be useful to get the title for the contents we are going to collect"
      ],
      "metadata": {
        "id": "iwPIdFXyhIpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(soup):\n",
        "    title = soup.find('h1').text.strip()\n",
        "    return title"
      ],
      "metadata": {
        "id": "IanxYCoOhko6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Text\n",
        "We are going to extract teh content of each page ignoring non-text. We do this targeting the paragraph tags `<p>`. The function will be given a soup (parsed html script), and it will write the contents of all the paragraphs in a text file."
      ],
      "metadata": {
        "id": "SRsxcS6BJ7Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = get_soup('https://www.uscis.gov/working-in-the-united-states/temporary-workers/e-1-treaty-traders')\n",
        "acc_headers = soup.find_all('div',class_='accordion__header cke-active')\n",
        "for i in acc_headers:\n",
        "    print(i['class'],i.get_text)"
      ],
      "metadata": {
        "id": "VPymYePHRpao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99ac606-da91-4b27-a511-fae7e1a0d9bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">Who May File for Change of Status to E-1 Classification</div>>\n",
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">How to Obtain E-1 Classification if Outside the United States</div>>\n",
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">General Qualifications of a Treaty Trader</div>>\n",
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">General Qualifications of the Employee of a Treaty Trader</div>>\n",
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">Period of Stay</div>>\n",
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">Terms and Conditions of E-1 Status</div>>\n",
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">Family of E-1 Treaty Traders and Employees</div>>\n",
            "['accordion__header', 'cke-active'] <bound method PageElement.get_text of <div class=\"accordion__header cke-active\" tabindex=\"0\">More Information</div>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_row(soup):\n",
        "    title = get_title(soup)\n",
        "    text = ''\n",
        "    main = soup.find('main')\n",
        "\n",
        "    for element in main.find_all():\n",
        "        if element.name == 'ul'or element.name == 'p':\n",
        "            text += element.get_text() + '\\n'\n",
        "        elif element.has_attr('class'):\n",
        "            if len(element['class'])>1:\n",
        "                if element['class'][0]=='accordion__header':\n",
        "                    text += element.get_text() + '\\n'\n",
        "\n",
        "    row = {'title': title, 'text': text}\n",
        "    return row"
      ],
      "metadata": {
        "id": "_svc1HZFJ7Cp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraper\n",
        "The scraper is going to visit all the relative urls and extract the useful contents from the paragraphs of each page. It will write the text on an output file, a csv.\n",
        "\n",
        "When you open a file you usually use with open(), this method will automatically close the file after you are done reading or writing. Open takes three attributes, the file name, the mode, and the encoding (automatic). You are usually reading or writing on a file, `r` will select reading mode, `w` will select writing mode. It is worth mentioning the modes:\n",
        "*   w+: Opens a file in read and write mode. It creates a new file if it does not exist, if it exists, it erases the contents of the file and the file pointer starts from the beginning.\n",
        "*   rw+: Opens a file in read and write mode. File pointer starts at the beginning of the file."
      ],
      "metadata": {
        "id": "Pg55IclULH5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scraper(base_url, output_file):\n",
        "    visited = set()\n",
        "    to_visit = [base_url]\n",
        "    columns = ['title', 'text']\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "    with open(output_file, 'w', newline = '') as f: # w for write mode\n",
        "        writer = csv.DictWriter(f, fieldnames = columns, dialect = 'unix')\n",
        "        writer.writeheader()\n",
        "        i=0\n",
        "        while to_visit:\n",
        "            print(i)\n",
        "            i+=1\n",
        "            # get url from to_visit\n",
        "            url = to_visit.pop() # removes and returns last element of the list\n",
        "            # confirm it is not in visited if it is skip to next iteration using continue\n",
        "            if url in visited:\n",
        "                continue\n",
        "            # add to visited\n",
        "            visited.add(url)\n",
        "            # get soup\n",
        "            soup = get_soup(url)\n",
        "            # get page title and text from soup and create a new row which has dict format\n",
        "            row = get_row(soup)\n",
        "            # write new row in the csv\n",
        "            writer.writerow(row)\n",
        "            print(row)\n",
        "            # get links from soup\n",
        "            links = get_links(soup, base_url)\n",
        "            # append links to to_visit list if they are not in the visited set\n",
        "            to_visit.extend(link for link in links if link not in visited)\n",
        "            if i==50:\n",
        "                continue\n",
        "scraper(base_url, output_file)"
      ],
      "metadata": {
        "id": "B-KdJppBMIQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scraper(base_url, output_file):\n",
        "    visited = set()\n",
        "    to_visit = [base_url]\n",
        "    columns = ['title', 'text']\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    i=0\n",
        "    while to_visit:\n",
        "        print(i)\n",
        "        i+=1\n",
        "        # get url from to_visit\n",
        "        url = to_visit.pop() # removes and returns last element of the list\n",
        "        # confirm it is not in visited if it is skip to next iteration using continue\n",
        "        if url in visited:\n",
        "            continue\n",
        "        # add to visited\n",
        "        visited.add(url)\n",
        "        # get soup\n",
        "        soup = get_soup(url)\n",
        "        # get page title and text from soup and create a new row which has dict format\n",
        "        row = get_row(soup)\n",
        "        # write new row in the csv\n",
        "        df = df.append(row, ignore_index = True)\n",
        "        # get links from soup\n",
        "        links = get_links(soup, base_url)\n",
        "        # append links to to_visit list if they are not in the visited set\n",
        "        to_visit.extend(link for link in links if link not in visited)\n",
        "\n",
        "    df.to_csv('text.csv', encoding='utf-8')\n",
        "    return df\n",
        "df = scraper(base_url, output_file)"
      ],
      "metadata": {
        "id": "0yb6SEdR8aKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('text.csv', encoding='utf-8', separator)"
      ],
      "metadata": {
        "id": "WmtdO1RG-oht"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FASTAPI intro\n",
        "Web framework for building APIs with Python. To run a FastAPI application we need a server program like Uvicorn. Uvicorn is an ASGI server program. It will let us run a server manually, our application will that way run in a remote server machine."
      ],
      "metadata": {
        "id": "wNQZ0JT1Nbwq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOHHFv1OvqSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}